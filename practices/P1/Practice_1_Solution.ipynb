{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice 1 (solution) - Text processing and topic modelling",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZIl1U-b7AzU"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 1:** Text processing and topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1vQPzjM9r8c"
      },
      "source": [
        "# **Text processing**\n",
        "---\n",
        "The text processing phase is a preliminary stage where the text to be manipulated is processed to be ready for subsequent analysis.\n",
        "\n",
        "Text processing usually entails several steps that could possibly include:\n",
        "- **Language Identification**: identifying the language of a given text.\n",
        "- **Tokenization**: splitting a given text in several sentences/words. \n",
        "- **Dependency tree parsing:** analyzing the depencies between words composing the text.\n",
        "- **Stemming/Lemmatization:** obtain the root form for each word in text.\n",
        "- **Stopword removal**: removing words that are si commonly used that they carry very little useful information.\n",
        "- **Part of Speech Tagging:** given a word, retrieve its part of speech (proper noun, common noun or verb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2dHRmrPB22r"
      },
      "source": [
        "### Language Identification\n",
        "\n",
        "| Text                                                                                                                                | Language Code |\n",
        "|-------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
        "| The \"Deep Natural Language Processing\" course is offered during the first semester of the second year at Politecnico di Torino      | `EN`            |\n",
        "| Il corso \"Deep Natural Language Processing\" viene impartito al Politecnico di Torino durante il primo semestre del secondo anno.    | `IT`            |\n",
        "| Le cours \"Deep Natural Language Processing\" est enseigné au Politecnico di Torino pendant le premier semestre de la deuxième année. | `FR`            |\n",
        "\n",
        "**Language Identification** is a crucial prelimiary step because each language has its own characteristics. The knowledge of the main language associated to a given text could be beneficial for all subsequent steps in text processing pipeline.\n",
        "\n",
        "The data collection used in this first part of the practice is provided [here](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P1/langid_dataset.csv) - [source: Kaggle](https://www.kaggle.com/martinkk5575/language-detection)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ej_dfjm2srd"
      },
      "source": [
        "# Exercise 1:\n",
        "\n",
        "Benchmark different language-detection algorithm by computing the accuracy of each approach:\n",
        "- [FastText](https://pypi.org/project/fastlangid/)\n",
        "- [LangID](https://github.com/saffsd/langid.py)\n",
        "- [langdetect](https://pypi.org/project/langdetect/)\n",
        "\n",
        "**Hint:** language code conversion: [iso639-lang](https://pypi.org/project/iso639-lang/)\n",
        "\n",
        "For each method report:\n",
        "- Accuracy\n",
        "- Average time per example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7xbkps3_mR"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/langid_dataset.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC6GK4uLD9y9"
      },
      "source": [
        "%%capture\n",
        "! pip install fastlangid\n",
        "! pip install langid\n",
        "! pip install langdetect\n",
        "! pip install iso639-lang\n",
        "! pip install pandas --upgrade\n",
        "! pip install numpy --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X46-pIKYF9K0"
      },
      "source": [
        "# dataset reading \n",
        "import pandas as pd\n",
        "from iso639 import Lang\n",
        "df_langid = pd.read_csv('langid_dataset.csv')\n",
        "print (df_langid)\n",
        "# convert to language codes\n",
        "df_langid['language'] = df_langid['language'].apply(lambda x: Lang(x).pt1)\n",
        "print (df_langid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iDDedfo90yI"
      },
      "source": [
        "! pip install sklearn # to compute accuracy\n",
        "! pip install tqdm # advancement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lPNcquP9o66"
      },
      "source": [
        "import langid\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "start = time.time()\n",
        "for index, row in tqdm(df_langid.iterrows()):\n",
        "    sentence = row[\"Text\"]\n",
        "    real_lang_code = row[\"language\"]\n",
        "    y_true.append(real_lang_code)\n",
        "    y_pred.append(langid.classify(sentence)[0])\n",
        "end = time.time()\n",
        "\n",
        "print (\"\\nlangid avg ms per example:\", (end-start)*1000/len(df_langid.index))\n",
        "print (\"\\nlangid accuracy:\", accuracy_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsqlKVg9_w7U"
      },
      "source": [
        "from langdetect import detect\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "start = time.time()\n",
        "for index, row in tqdm(df_langid.iterrows()):\n",
        "    sentence = row[\"Text\"]\n",
        "    real_lang_code = row[\"language\"]\n",
        "    y_true.append(real_lang_code)\n",
        "    try:\n",
        "        y_pred.append(detect(sentence))\n",
        "    except Exception as e:\n",
        "        y_pred.append(\"\")\n",
        "        print (e, \"\\n\")\n",
        "end = time.time()\n",
        "\n",
        "print (\"\\nlangdetect avg ms per example:\", (end-start)*1000/len(df_langid.index))\n",
        "print (\"\\nlangdetect accuracy:\", accuracy_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBa5LAnMA084"
      },
      "source": [
        "from fastlangid.langid import LID\n",
        "ft_langid = LID()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "start = time.time()\n",
        "for index, row in tqdm(df_langid.iterrows()):\n",
        "    sentence = row[\"Text\"]\n",
        "    real_lang_code = row[\"language\"]\n",
        "    y_true.append(real_lang_code)\n",
        "    try:\n",
        "        y_pred.append(ft_langid.predict(sentence))\n",
        "    except Exception as e:\n",
        "        y_pred.append(\"\")\n",
        "        print (e, \"\\n\")\n",
        "end = time.time()\n",
        "\n",
        "print (\"\\nlangdetect avg ms per example:\", (end-start)*1000/len(df_langid.index))\n",
        "print (\"\\nlangdetect accuracy:\", accuracy_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIOgOZy1ENh2"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "For English-written text, apply word-level tokenization. What is the average number of words per sentence?\n",
        "\n",
        "Implement word-tokenization using both [nltk](https://www.nltk.org/) and [spacy](https://spacy.io/). Report the results for both of them.\n",
        "\n",
        "For spaCy use the `en_core_web_sm` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsRSt5hmE5k7"
      },
      "source": [
        "%%capture\n",
        "! pip install --upgrade spacy\n",
        "! pip install nltk\n",
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaZ81BwIE80g"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "len_total = 0\n",
        "count_total = 0\n",
        "for index, row in tqdm(df_langid.iterrows()):\n",
        "    if row[\"language\"] == \"en\":\n",
        "        len_total += len(word_tokenize(row[\"Text\"]))\n",
        "        count_total += 1\n",
        "print (\"\\nAvg. len in #words - NLTK:\", len_total/count_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4iIkXK3HRkE"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "len_total = 0\n",
        "count_total = 0\n",
        "for index, row in tqdm(df_langid.iterrows()):\n",
        "    if row[\"language\"] == \"en\":\n",
        "        doc = nlp(row[\"Text\"])\n",
        "        len_total += len(doc)\n",
        "        count_total += 1\n",
        "print (\"\\nAvg. len in #words - spaCy:\", len_total/count_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySnw80QnIf2p"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "Use spacy to parse the dependency tree of a **randomly selected** sentence. You can both use English sentences or your native language (if supported in [spaCy](https://spacy.io/usage/models/)). Use [displaCy](https://explosion.ai/demos/displacy) to visualize the result in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q474xseqTZSL"
      },
      "source": [
        "%%capture\n",
        "! pip install --upgrade spacy\n",
        "! python -m spacy download ro_core_news_sm\n",
        "! python -m spacy download en_core_web_sm\n",
        "! python -m spacy download pt_core_news_sm\n",
        "! python -m spacy download it_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMCS5HTxSvSB"
      },
      "source": [
        "import random\n",
        "import spacy\n",
        "sentences_pool = []\n",
        "lang_code = \"en\"\n",
        "if lang_code ==\"en\":\n",
        "    nlp = spacy.load(lang_code + '_core_web_sm')\n",
        "else:\n",
        "    nlp = spacy.load(lang_code + '_core_news_sm')\n",
        "\n",
        "for index, row in df_langid.iterrows():\n",
        "    if row[\"language\"] == lang_code:\n",
        "        sentences_pool.append(row[\"Text\"])\n",
        "\n",
        "s = random.choice(sentences_pool)\n",
        "doc = nlp(s)\n",
        "spacy.displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAO1E6adVMKW"
      },
      "source": [
        "# Exercise 4\n",
        "For the same sentence selected in the previous step apply all the following steps:\n",
        "1. Lemmatization: convert each word to its root form.\n",
        "2. Stopword removal: remove language-specific stopwords.\n",
        "3. Part of Speech Tagging: for each word in the sentence display its part-of-speech.\n",
        "\n",
        "For each step, print the resulting list on the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z7yUuYIW-w8"
      },
      "source": [
        "#Lemmatization\n",
        "doc = nlp(s)\n",
        "lemmas = []\n",
        "for w in doc:\n",
        "    lemmas.append(w.lemma_)\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK3tqwxrWRmZ"
      },
      "source": [
        "#Stopword removal\n",
        "clean_sentence = [] \n",
        "for w in doc:\n",
        "    if not w.is_stop:\n",
        "        clean_sentence.append(w.text)\n",
        "print(\" \".join(clean_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4xWHxayWRpH"
      },
      "source": [
        "#PoS Tagging\n",
        "for w in doc:\n",
        "    print (w.text, w.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4-wJHnOZnwC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4KXiBkZrta"
      },
      "source": [
        "# **Occurrence-based text representation - TF-IDF**\n",
        "\n",
        "---\n",
        "\n",
        "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It allows to create occurrence-based vector representation for each document.\n",
        "\n",
        "# Exercise 5\n",
        "Use TF-IDF to vectorize each sentence in the original data collection. You can choose your preferred implementation for TF-IDF vectorization. It is also available on [SciKit-Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rpC2R8MawAT"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = df_langid[\"Text\"].tolist()\n",
        "language_labels = df_langid[\"language\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7L162ayglUq"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4NTwm6Qbpvx"
      },
      "source": [
        "# Exercise 6\n",
        "\n",
        "Build a supervised multi-class language detector using as features the vector obtained by TF-IDF representation. Use 80% of the data to train the language detector and 20% of the data for assessing its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HivUJsnCcUco"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, language_labels , test_size=0.20)\n",
        "mlp = MLPClassifier(verbose=True)\n",
        "mlp = mlp.fit(X_train, y_train)\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kUlaYXsxbTV"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, language_labels , test_size=0.20)\n",
        "dt = DecisionTreeClassifier()\n",
        "dt = dt.fit(X_train, y_train)\n",
        "y_pred = dt.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AHlGUHuaqjW"
      },
      "source": [
        "# **Topic Modelling**\n",
        "---\n",
        "Occurrence-based representations are high-dimensional, what is the dimension of the generated TF-IDF vector representation?\n",
        "Topic modelling focuses on caturing latent topics in large document corpora.\n",
        "\n",
        "The data collection used in this second part of the practice is provided [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv) - [source: Zenodo](https://zenodo.org/record/4282522#.YVdCXcbOOpd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZbbPhC6cAO8"
      },
      "source": [
        "# Exercise 7\n",
        "\n",
        "Latent Semantic Indexing (LSI) models underlying concepts by using SVD (Singular Value Decomposition).\n",
        "\n",
        "Use [gensim](https://radimrehurek.com/gensim/) library to:\n",
        "1. Create a corpus composed of the headlines contained in the data collection.\n",
        "2. Generate a [dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) to create a word -> id mapping (required by LSI module).\n",
        "3. Using the dictionary, preprocess the corpus to obtain the representation required for LSI model training ([documentation here](https://radimrehurek.com/gensim/models/lsimodel.html)).\n",
        "4. Inspect the top-5 topics generated by the LSI model for the analysed corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwkHsT8oft_d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HMsbJqHd_nD"
      },
      "source": [
        "import pandas as pd\n",
        "df_tmodelling = pd.read_csv('CovidFake_filtered.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK7rZOr1fUpl"
      },
      "source": [
        "# generating the corpus \n",
        "corpus = df_tmodelling[\"headlines\"].tolist()\n",
        "corpus = [s.split() for s in corpus]\n",
        "print (corpus[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHZhHqJTepKE"
      },
      "source": [
        "# constructing dictionary and processing the corpus\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "tm_dict = Dictionary(corpus)\n",
        "processed_corpus = [tm_dict.doc2bow(text) for text in corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6KfygwTeVeU"
      },
      "source": [
        "# training LSI model\n",
        "from gensim.models import LsiModel\n",
        "model = LsiModel(processed_corpus, id2word=tm_dict)\n",
        "model.print_topics(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zcmfh5UkdZm"
      },
      "source": [
        "# Exercise 8 (Optional)\n",
        "\n",
        "The top-scored words contributing to each topic (if no stopword removal is applied) are english common words (e.g., *to, for, in, of, on*..). Moreover, missing punctuation removal could be critical for topic identification. Repeat the same procedure of Ex. 7 by adding preliminary preprocessing step to:\n",
        "1. **remove stopwords**\n",
        "2. **strip punctuation**\n",
        "3. **lowercase all words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoJoiZhujDik"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "rs_corpus = [[w.lower() for w in s if w.lower() not in stop_words] for s in corpus]\n",
        "rs_corpus = [[w.translate(str.maketrans('', '', string.punctuation)) for w in s] for s in rs_corpus]\n",
        "print(rs_corpus[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77u-So3fnRLw"
      },
      "source": [
        "rs_tm_dict = Dictionary(corpus)\n",
        "rs_processed_corpus = [rs_tm_dict.doc2bow(text) for text in rs_corpus]\n",
        "rs_model = LsiModel(rs_processed_corpus, id2word=rs_tm_dict)\n",
        "rs_model.print_topics(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zssKPqSdyYgY"
      },
      "source": [
        "# Exercise 9 (Optional)\n",
        "\n",
        "Leveraging the same corpus used for LSI model generation, apply LDA modelling setting the number of topics to 5. Display the words most contributing to the those topics according to the LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp3yOxxsyX7y"
      },
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "lda = LdaModel(rs_processed_corpus, id2word=rs_tm_dict, num_topics=3)\n",
        "lda.print_topics(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1YVohPi7R6K"
      },
      "source": [
        "# Exercise 10 (Optional)\n",
        "\n",
        "Using [pyLDAvis]() library build an interactive visualization for the trained LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhe_AoaU-AsJ"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gEdP9uh9zPL"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "lda_display = gensimvis.prepare(lda, rs_processed_corpus, rs_tm_dict, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
