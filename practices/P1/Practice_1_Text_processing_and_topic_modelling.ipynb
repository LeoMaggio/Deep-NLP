{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice 1 - Text processing and topic modelling",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoMaggio/Deep-NLP/blob/main/practices/P1/Practice_1_Text_processing_and_topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZIl1U-b7AzU"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 1:** Text processing and topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1vQPzjM9r8c"
      },
      "source": [
        "# **Text processing**\n",
        "---\n",
        "The text processing phase is a preliminary stage where the text to be manipulated is processed to be ready for subsequent analysis.\n",
        "\n",
        "Text processing usually entails several steps that could possibly include:\n",
        "- **Language Identification**: identifying the language of a given text.\n",
        "- **Tokenization**: splitting a given text in several sentences/words. \n",
        "- **Dependency tree parsing:** analyzing the depencies between words composing the text.\n",
        "- **Stemming/Lemmatization:** obtain the root form for each word in text.\n",
        "- **Stopword removal**: removing words that are si commonly used that they carry very little useful information.\n",
        "- **Part of Speech Tagging:** given a word, retrieve its part of speech (proper noun, common noun or verb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2dHRmrPB22r"
      },
      "source": [
        "### Language Identification\n",
        "\n",
        "| Text                                                                                                                                | Language Code |\n",
        "|-------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
        "| The \"Deep Natural Language Processing\" course is offered during the first semester of the second year at Politecnico di Torino      | `EN`            |\n",
        "| Il corso \"Deep Natural Language Processing\" viene impartito al Politecnico di Torino durante il primo semestre del secondo anno.    | `IT`            |\n",
        "| Le cours \"Deep Natural Language Processing\" est enseigné au Politecnico di Torino pendant le premier semestre de la deuxième année. | `FR`            |\n",
        "\n",
        "**Language Identification** is a crucial prelimiary step because each language has its own characteristics. The knowledge of the main language associated to a given text could be beneficial for all subsequent steps in text processing pipeline.\n",
        "\n",
        "The data collection used in this first part of the practice is provided [here](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P1/langid_dataset.csv) - [source: Kaggle](https://www.kaggle.com/martinkk5575/language-detection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ej_dfjm2srd"
      },
      "source": [
        "# Exercise 1:\n",
        "\n",
        "Benchmark different language-detection algorithm by computing the accuracy of each approach:\n",
        "- [FastText](https://pypi.org/project/fastlangid/)\n",
        "- [LangID](https://github.com/saffsd/langid.py)\n",
        "- [langdetect](https://pypi.org/project/langdetect/)\n",
        "\n",
        "**Hint:** language code conversion: [iso639-lang](https://pypi.org/project/iso639-lang/)\n",
        "\n",
        "For each method report:\n",
        "- Accuracy\n",
        "- Average time per example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8t1hlXkGpLh"
      },
      "source": [
        "!pip install iso639-lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7xbkps3_mR"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/langid_dataset.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EpMqv6fREouD",
        "outputId": "9b1061a3-f00b-4e2a-902a-e4d9840c65a6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from iso639 import Lang\n",
        "import time\n",
        "\n",
        "df = pd.read_csv('langid_dataset.csv')\n",
        "df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
              "      <td>Estonian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
              "      <td>Swedish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
              "      <td>Tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
              "      <td>Dutch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  language\n",
              "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
              "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
              "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
              "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
              "4  de spons behoort tot het geslacht haliclona en...     Dutch"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGypEo0tFfei"
      },
      "source": [
        "X = df['Text']\n",
        "y = df['language']"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VrpgY5uB6mK"
      },
      "source": [
        "## 1.1 FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-509xNcU0tcU"
      },
      "source": [
        "!pip install fastlangid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yArRH5VFB1Gm",
        "outputId": "59158f7c-aac2-4962-8d37-02bcfe6e9e61"
      },
      "source": [
        "from fastlangid.langid import LID\n",
        "\n",
        "langid = LID()\n",
        "acc = 0\n",
        "start = time.time()\n",
        "results = langid.predict(X)\n",
        "elapsed_time = time.time() - start\n",
        "for i, pred in enumerate(results):\n",
        "  lg = Lang(y[i])\n",
        "  if pred == lg.pt1:\n",
        "    acc += 1\n",
        "\n",
        "acc = acc / len(y)\n",
        "avg_time = elapsed_time / len(y)\n",
        "print(f\"Accuracy: {acc}\")\n",
        "print(f\"Average time per example: {avg_time}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9231818181818182\n",
            "Average time per example: 0.00017676564780148593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQG8mpfIIyJc"
      },
      "source": [
        "## 1.2 LangID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRBOJnL9Kmmk"
      },
      "source": [
        "!pip install langid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UudpImTfI3Mn",
        "outputId": "69bbd4c9-6f83-44e8-e26e-3395e2cd3582"
      },
      "source": [
        "from langid.langid import LanguageIdentifier, model\n",
        "\n",
        "identifier = LanguageIdentifier.from_modelstring(model)\n",
        "acc = 0\n",
        "results = []\n",
        "\n",
        "start = time.time()\n",
        "for i, text in enumerate(X):\n",
        "  results.append(identifier.classify(text))\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "for i, pred in enumerate(results):\n",
        "  lg = Lang(y[i])\n",
        "  if pred == lg.pt1:\n",
        "    acc += 1\n",
        "\n",
        "acc = acc / len(y)\n",
        "avg_time = elapsed_time / len(y)\n",
        "print(f\"Accuracy: {acc}\")\n",
        "print(f\"Average time per example: {avg_time}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Average time per example: 0.003210650606588884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvCl7pPHMGAv"
      },
      "source": [
        "## 1.3 langdetect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5qcHFz3MQip"
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5pj12MQMVWq",
        "outputId": "dd56bff4-339b-4cbc-f221-07072bc8e8f6"
      },
      "source": [
        "from langdetect import detect\n",
        "\n",
        "acc = 0\n",
        "results = []\n",
        "\n",
        "start = time.time()\n",
        "for i, text in enumerate(X):\n",
        "  try:\n",
        "    results.append(detect(text))\n",
        "  except:\n",
        "    results.append(\"\")\n",
        "    print(\"This text throws and error: \", text)\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "for i, pred in enumerate(results):\n",
        "  lg = Lang(y[i])\n",
        "  if pred == lg.pt1:\n",
        "    acc += 1\n",
        "\n",
        "acc = acc / len(y)\n",
        "avg_time = elapsed_time / len(y)\n",
        "print(f\"Accuracy: {acc}\")\n",
        "print(f\"Average time per example: {avg_time}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This text throws and error:  ﺩﺍﻭﺩﺍﺳﻪ ﻓﻀﯿﻠﺖ ﭘﻪ ﺍﺣﺎﺩﯾﺜﻮﮐﻲ  – ﺣﻤﺮﺍﻥ ﺭﻭﺍﯾﺖ ﮐﻮﯼ ﭼﯥ ﯾﻮﻩ ﻭﺭځ ﺣﻀﺮﺕ ﻋﺜﻤﺎﻥ ﺑﻦ ﻋﻔﺎﻥ ﺭﺿﯽ ﺍﻟﻠﻪ ﻋﻨﻪ ﭘﻪ ښﻪ ډﻭﻝ ﺳﺮﻩ ﺍﻭﺩﺱ ﺗﺎﺯﻩ ﮐړ ﺍﻭﺑﯿﺎﯾﯽ ﻭﻓﺮﻣﺎﯾﻞ ﻣﺎﺭﺳﻮﻝ ﺍﻟﻠﻪ ﺻﻠﯽ ﺍﻟﻠﻪ ﻋﻠﯿﻪ ﻭﺳﻠﻢ ﭘﺮﺍﻭﺩﺍﺳﻪ ﻭﻟﯿﺪﯼ ﭘﻪ ښﻪ ډﻭﻝ ﺳﺮﻩ ﺋﯥ ﺍﻭﺩﺱ ﺗﺎﺯﻩ ﮐړ ﺍﻭﺑﯿﺎﯾﯽ ﻭﻓﺮﻣﺎﯾﻞ\n",
            "This text throws and error:                                           \n",
            "This text throws and error:   – ﺩﺣﻀﺮﺕ ﺍﺑﻮﻫﺮﯾﺮﻩ ﺭﺿﯽ ﺍﻟﻠﻪ ﻋﻨﻪ څﺨﻪ ﺭﻭﺍﯾﺖ ﺩﯼ ﭼﯥ ﻣﺎﺩﻧﺒﯽ ﮐﺮﯾﻢ ﺻﻠﯽ ﺍﻟﻠﻪ ﻋﻠﯿﻪ ﻭﺳﻠﻢ ﻧﻪ ﻭﺍﻭﺭﯾﺪﻝ ﭼﯥ ﺩﺍﺍﻣﺖ ﺑﻪ ﺩﻗﯿﺎﻣﺖ ﭘﻪ ﻭﺭځ ﺑﺎﻧﺪﯼ ﺭﺍﻭﻏﻮښﺘﻞ ﺷﻲ ﭼﯥ ﺩﺍﻭﺩﺍﺳﻪ ﻟﻪ ﮐﺒﻠﻪ ﺑﻪ ﺩﺩﻭﯼ ﻻﺳﻮﻧﻪ ﭘښﯥ ﺍﻭﻣﺨﻮﻧﻪ ﻧﻮﺭﺍﻧﻲ ﺍﻭﺭﻭښﺎﻧﻪ ﻭﻱ څﻮﮎ ﭼﯥ ﺧﭙﻠﻪ ﺭﻭښﻨﺎﯾﯽ ﺯﯾﺎﺗﻮﯼ ﻧﻮﺯﯾﺎﺗﯽ ﺩﯼ ﮐړﻱﺭﻭﺍﻩ ﺑﺨﺎﺭﻱ ﺍﻟﺘﺮﻏﯿﺐ ﻭﺍﻟﺘﺮﻫﯿﺐ ﻟﻮﻣړﯼ ټﻮﮎ  ﭘﺎڼﻪ ﺣﺪﯾﺚ   ﻟﯿﮑﻮﺍﻝ ﺣﺎﻓﻆ ﺯﮐﻲ ﺍﻟﺪﯾﻦ ﻋﺒﺪﺍﻟﻌﻈﯿﻢ ﺑﻦ ﻋﺒﺪﺍﻟﻘﻮﻱ ﺍﻟﻤﻨﺬﺭﯼ ﺍﻟﻤﺘﻮﻓﯽ  ﻫﻖ\n",
            "Accuracy: 0.8433636363636363\n",
            "Average time per example: 0.007015312606638128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIOgOZy1ENh2"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "For English-written text, apply word-level tokenization. What is the average number of words per sentence?\n",
        "\n",
        "Implement word-tokenization using both [nltk](https://www.nltk.org/) and [spacy](https://spacy.io/). Report the results for both of them.\n",
        "\n",
        "For spaCy use the `en_core_web_sm` model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dq7N9tqSOGb"
      },
      "source": [
        "## 2.1 Natural Language Toolkit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ph9mCx00sQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3649fa62-1cca-4f18-fb8b-2cf134d37432"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySnw80QnIf2p"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "Use spacy to parse the dependency tree of a **randomly selected** sentence. You can both use English sentences or your native language (if supported in [spaCy](https://spacy.io/usage/models/)). Use [displaCy](https://explosion.ai/demos/displacy) to visualize the result in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoFgoxwJ03_c"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAO1E6adVMKW"
      },
      "source": [
        "# Exercise 4\n",
        "For the same sentence selected in the previous step apply all the following steps:\n",
        "1. Lemmatization: convert each word to its root form.\n",
        "2. Stopword removal: remove language-specific stopwords.\n",
        "3. Part of Speech Tagging: for each word in the sentence display its part-of-speech.\n",
        "\n",
        "For each step, print the resulting list on the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4-wJHnOZnwC"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4KXiBkZrta"
      },
      "source": [
        "# **Occurrence-based text representation - TF-IDF**\n",
        "\n",
        "---\n",
        "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It allows to create occurrence-based vector representation for each document.\n",
        "\n",
        "# Exercise 5\n",
        "Use TF-IDF to vectorize each sentence in the original data collection. You can choose your preferred implementation for TF-IDF vectorization. It is also available on [SciKit-Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7L162ayglUq"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4NTwm6Qbpvx"
      },
      "source": [
        "# Exercise 6\n",
        "\n",
        "Build a supervised multi-class language detector using as features the vector obtained by TF-IDF representation. Use 80% of the data to train the language detector and 20% of the data for assessing its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1HT2roy1DQu"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AHlGUHuaqjW"
      },
      "source": [
        "# **Topic Modelling**\n",
        "\n",
        "Occurrence-based representations are high-dimensional, what is the dimension of the generated TF-IDF vector representation?\n",
        "Topic modelling focuses on caturing latent topics in large document corpora.\n",
        "\n",
        "The data collection used in this second part of the practice is provided [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv) - [source: Zenodo](https://zenodo.org/record/4282522#.YVdCXcbOOpd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZbbPhC6cAO8"
      },
      "source": [
        "# Exercise 7\n",
        "\n",
        "Latent Semantic Indexing (LSI) models underlying concepts by using SVD (Singular Value Decomposition).\n",
        "\n",
        "Use [gensim](https://radimrehurek.com/gensim/) library to:\n",
        "1. Create a corpus composed of the headlines contained in the data collection.\n",
        "2. Generate a [dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) to create a word -> id mapping (required by LSI module).\n",
        "3. Using the dictionary, preprocess the corpus to obtain the representation required for LSI model training ([documentation here](https://radimrehurek.com/gensim/models/lsimodel.html)).\n",
        "4. Inspect the top-5 topics generated by the LSI model for the analysed corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwkHsT8oft_d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoJAZsw11Gih"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zcmfh5UkdZm"
      },
      "source": [
        "# Exercise 8 (Optional)\n",
        "\n",
        "The top-scored words contributing to each topic (if no stopword removal is applied) are english common words (e.g., *to, for, in, of, on*..). Repeat the same procedure of Ex. 7 by adding a preliminary preprocessing step to **remove stopwords**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz2NUP6O1JAK"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zssKPqSdyYgY"
      },
      "source": [
        "# Exercise 9 (Optional)\n",
        "\n",
        "Leveraging the same corpus used for LSI model generation, apply LDA modelling setting the number of topics to 5. Display the words most contributing to the those topics according to the LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzCXdEUW1MHs"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBhdPyiKDNZ0"
      },
      "source": [
        "# Exercise 10 (Optional)\n",
        "\n",
        "Using [pyLDAvis]() library build an interactive visualization for the trained LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aM6yrdPDREK"
      },
      "source": [
        "# your code"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}