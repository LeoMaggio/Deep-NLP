{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice 4 - NER and Intent Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoMaggio/Deep-NLP/blob/main/practices/P4/Practice_4_NER_and_Intent_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHLvIkbUsjZ"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Named Entities Recognition & Intent Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQde2M-U4KV"
      },
      "source": [
        "## Named Entities Recognition\n",
        "\n",
        "Named-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
        "\n",
        "![https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg](https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg)   \n",
        "\n",
        "Text domain is **crucial** while recognizing entities (political, medical, food...)\n",
        "\n",
        "In this practice you will:\n",
        "- Experiment with pre-trained models to extract entities from text\n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9elJruvpWAQ_"
      },
      "source": [
        "### **Question 1: data preparation**\n",
        "\n",
        "The data collection is available [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt). \n",
        "This dataset was presented in [1][2] and consists of a set of manually annotated Wikipedia text. The data already in [CONLL](https://simpletransformers.ai/docs/ner-data-formats/#text-file-in-conll-format) format. Please read carefully before proceeding with data parsing.\n",
        "\n",
        "You need to extract clean sentences (no annotation) and, for each sentence, text associated to each entity:     \n",
        "- `sentences`: list of sentences\n",
        "- `annotations`: list of list of entities (both string and class information). E.g., `[[('010', 'MISC'), ('Japanese', 'MISC'), ('The Mad Capsule Markets', 'ORG')], [('Osc-Dis', 'MISC'), ('Introduction 010', 'MISC'), ('Come', 'MISC')], ...]`. You can remove I- prefix because the data collection does not actually cotains valuable prefixes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[1] Balasuriya, Dominic, et al. \"Named entity recognition in wikipedia.\"\n",
        "    Proceedings of the 2009 Workshop on The People's Web Meets NLP: Collaboratively Constructed Semantic Resources. Association for Computational Linguistics, 2009.\n",
        "\n",
        "[2] Nothman, Joel, et al. \"Learning multilingual named entity recognition\n",
        "    from Wikipedia.\" Artificial Intelligence 194 (2013): 151-175 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeGD-RtlV_0k"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK19r9la4jUd"
      },
      "source": [
        "import re\n",
        "\n",
        "def split_text_label(filename):\n",
        "    f = open(filename)\n",
        "    split_labeled_text = []\n",
        "    split_text = []\n",
        "    split_labels = []\n",
        "    sentence = []\n",
        "    annotations = []\n",
        "    text = \"\"\n",
        "    for line in f:\n",
        "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "             if len(sentence) > 0:\n",
        "                 split_labeled_text.append(sentence)\n",
        "                 split_text.append(text)\n",
        "                 split_labels.append(annotations)\n",
        "                 sentence = []\n",
        "                 text = \"\"\n",
        "                 annotations = []\n",
        "             continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
        "        annotations.append((splits[0],splits[-1].lstrip(\"I-\").rstrip(\"\\n\")))\n",
        "        text = f\"{text} {splits[0]}\"\n",
        "        text = re.sub(r'\\s([?.!,)\"](?:\\s|$))', r'\\1', text.strip())\n",
        "    if len(sentence) > 0:\n",
        "        split_labeled_text.append(sentence)\n",
        "        split_text.append(text)\n",
        "        split_labels.append(annotations)\n",
        "        sentence = []\n",
        "        text = \"\"\n",
        "        annotations = []\n",
        "    return split_labeled_text, split_text, split_labels\n",
        "sentences_with_labels, sentences, ground_truths = split_text_label(\"wikigold.conll.txt\")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY3VJcC9Bvnf"
      },
      "source": [
        "### **Question 2: inference with spacy for entity recognition**\n",
        "\n",
        "Spacy models comes with built-in NER models. Instantiate a [spacy model](https://spacy.io/usage/models) for the english language and get, for each sentence in the data collection, its named entities extracted from the model.\n",
        "\n",
        "Given that, the provided data collection only contains a subset of spacy labels map all the classes not available in the data collection to the `MISC` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXMAjNvaIoj5"
      },
      "source": [
        "%%capture\n",
        "!pip install -U spacy \n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPO0aU1lqA7P"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyFAeI7JrL7g"
      },
      "source": [
        "def get_ents(doc):\n",
        "  sentence = []\n",
        "  if doc.ents:\n",
        "    for ent in doc.ents:\n",
        "      sentence.append((ent.text, ent.label_))\n",
        "  return sentence"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmkTyLQDrr-t"
      },
      "source": [
        "predictions_spacy = []\n",
        "for s in sentences:\n",
        "  doc = nlp(s)\n",
        "  doc_ents = get_ents(doc)\n",
        "  if doc_ents:\n",
        "    predictions_spacy.append(doc_ents)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf0NzZzjPheL"
      },
      "source": [
        "### **Question 3: compute metrics for evaluating NER**\n",
        "\n",
        "Use [eval4ner](https://github.com/cyk1337/eval4ner) to evaluate the spacy model for NER on the parsed dataset.\n",
        "\n",
        "**Note**: please use `pip install git+https://github.com/MorenoLaQuatra/eval4ner` to use a fixed version of the library. Before passing the parameter to the evaluation function, create a deepcopy of each variable:\n",
        "\n",
        "The issue has been already reported to the original author."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACNK6UU98VZr"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/MorenoLaQuatra/eval4ner"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlJM3PVa8SGS",
        "outputId": "95ec4251-b5d1-4a6d-903a-6436ead13a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "import eval4ner.muc as muc\n",
        "\n",
        "muc.evaluate_all(predictions_spacy, ground_truths * 1, sentences, verbose=True)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-9734a0fb6daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval4ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmuc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmuc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmuc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_spacy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truths\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/eval4ner/muc.py\u001b[0m in \u001b[0;36mevaluate_all\u001b[0;34m(predictions, golden_labels, texts, verbose)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \"\"\"\n\u001b[1;32m    258\u001b[0m     assert len(predictions) == len(golden_labels) == len(\n\u001b[0;32m--> 259\u001b[0;31m         texts), 'the counts of predictions/golden_labels/texts are not equal!'\n\u001b[0m\u001b[1;32m    260\u001b[0m     eval_metics = {\"precision\": 0,\n\u001b[1;32m    261\u001b[0m                    \u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: the counts of predictions/golden_labels/texts are not equal!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC19UMUZdJxc"
      },
      "source": [
        "### **Question 4: inference with transformers pipeline**\n",
        "\n",
        "Transformer-based models can be fine-tuned for token-level classification. The most relevant task in this class is NER. Use [transformers pipelines](https://huggingface.co/transformers/master/main_classes/pipelines.html#transformers.TokenClassificationPipeline) to recognize entities in the previous data collection. \n",
        "\n",
        "Evaluate the model using the same procedure of Q3.\n",
        "\n",
        "**Note:** the output of the pipeline differs with respect to spacy. Please be sure to process data correctly before running evaluation.\n",
        "\n",
        "**Note 2:** `ignore_labels` parameter could be useful to correctly parse entities.\n",
        "\n",
        "**Note 3:** `##` symbol is used when a token is a continuation of a previous one (Poli + ##TO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6pQuOdTZZVo"
      },
      "source": [
        "%%capture\n",
        "! pip install datasets transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8I-9jTBI_5F"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrJbaVgi_ja"
      },
      "source": [
        "## Intent Detection\n",
        "\n",
        "In data mining, intention mining or intent mining is the problem of determining a user's intention from logs of his/her behavior in interaction with a computer system, such as in search engines. Intent Detection is the identification and categorization of what a user online intended or wanted to find when they type or speak with a conversational agent (or a search engine).\n",
        "\n",
        "![https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png](https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png)\n",
        "\n",
        "Data source (ATIS dataset): https://github.com/yvchen/JointSLU ; https://www.kaggle.com/siddhadev/atis-dataset-clean/home\n",
        "\n",
        "Use provided train/dev/test split accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L6-2ABir0yS"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.train.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.dev.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.test.csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nJxxjTj8Q9"
      },
      "source": [
        "### **Question 5: two-step classification model**\n",
        "\n",
        "Train a classification model to identify the intent from sentence text. The model should leverage on pretrained BERT model to generate features for each sentence (No-finetuning).\n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true)\n",
        "\n",
        "\n",
        "Assess the performance of the generated model by using the **classification accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STJSjSovq46n"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlOYXd_755CC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBTltGYv6FCT"
      },
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.train.csv')\n",
        "df_dev = pd.read_csv('https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.dev.csv')\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.test.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11S-W-hd_IJj",
        "outputId": "1f17adb9-b5c8-499d-933e-e2cd2507c91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "df_train"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tokens</th>\n",
              "      <th>slots</th>\n",
              "      <th>intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train-00001</td>\n",
              "      <td>BOS what is the cost of a round trip flight fr...</td>\n",
              "      <td>O O O O O O O B-round_trip I-round_trip O O B-...</td>\n",
              "      <td>atis_airfare</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train-00002</td>\n",
              "      <td>BOS now i need a flight leaving fort worth and...</td>\n",
              "      <td>O O O O O O O B-fromloc.city_name I-fromloc.ci...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train-00003</td>\n",
              "      <td>BOS i need to fly from kansas city to chicago ...</td>\n",
              "      <td>O O O O O O B-fromloc.city_name I-fromloc.city...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train-00004</td>\n",
              "      <td>BOS what is the meaning of meal code s EOS</td>\n",
              "      <td>O O O O O O B-meal_code I-meal_code I-meal_code O</td>\n",
              "      <td>atis_abbreviation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train-00005</td>\n",
              "      <td>BOS show me all flights from denver to pittsbu...</td>\n",
              "      <td>O O O O O O B-fromloc.city_name O B-toloc.city...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4269</th>\n",
              "      <td>train-04270</td>\n",
              "      <td>BOS show me flights from baltimore to philadel...</td>\n",
              "      <td>O O O O O B-fromloc.city_name O B-toloc.city_n...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4270</th>\n",
              "      <td>train-04271</td>\n",
              "      <td>BOS i need information on flights from indiana...</td>\n",
              "      <td>O O O O O O O B-fromloc.city_name O B-toloc.ci...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4271</th>\n",
              "      <td>train-04272</td>\n",
              "      <td>BOS what flights are there from phoenix to mil...</td>\n",
              "      <td>O O O O O O B-fromloc.city_name O B-toloc.city...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4272</th>\n",
              "      <td>train-04273</td>\n",
              "      <td>BOS please show me the flights from atlanta to...</td>\n",
              "      <td>O O O O O O O B-fromloc.city_name O B-toloc.ci...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4273</th>\n",
              "      <td>train-04274</td>\n",
              "      <td>BOS show me the flights from cleveland to memp...</td>\n",
              "      <td>O O O O O O B-fromloc.city_name O B-toloc.city...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4274 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               id  ...             intent\n",
              "0     train-00001  ...       atis_airfare\n",
              "1     train-00002  ...        atis_flight\n",
              "2     train-00003  ...        atis_flight\n",
              "3     train-00004  ...  atis_abbreviation\n",
              "4     train-00005  ...        atis_flight\n",
              "...           ...  ...                ...\n",
              "4269  train-04270  ...        atis_flight\n",
              "4270  train-04271  ...        atis_flight\n",
              "4271  train-04272  ...        atis_flight\n",
              "4272  train-04273  ...        atis_flight\n",
              "4273  train-04274  ...        atis_flight\n",
              "\n",
              "[4274 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwuI7ofl6sA8",
        "outputId": "66f9776c-00ce-4f5c-b262-1bb9ee4d8c43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%capture\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY1c-rCW77du"
      },
      "source": [
        "tokenized = df_train.tokens.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZrsN1Sb7hDD"
      },
      "source": [
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXpiIhRL9Ma9"
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6h6sff99S-D"
      },
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKIHBXB1-4Oj"
      },
      "source": [
        "train_features = last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM2hJczx-5Wb"
      },
      "source": [
        "train_labels = df_train.intent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xrNtQTq5WW"
      },
      "source": [
        "### **Question 6: two-step classification model**\n",
        "\n",
        "Train a new BERT model for the task of [sequence classification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) (include BERT fine-tuning).  \n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true)\n",
        "\n",
        "Assess the performance of the generated model by using the **classification accuracy**.\n",
        "\n",
        "Which model has better performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlKATw5sJAY"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}